{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is to clone and resubmit datasets of 20 lab study participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time\n",
    "from metaspace.sm_annotation_utils import SMInstance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_dataset(sm, ds_id):\n",
    "    gql = sm._gqclient\n",
    "    result = gql.query(\n",
    "        \"\"\"\n",
    "        query editDatasetQuery($id: String!) {\n",
    "          dataset(id: $id) {\n",
    "            id\n",
    "            name\n",
    "            metadataJson\n",
    "            configJson\n",
    "            isPublic\n",
    "            inputPath\n",
    "            group { id }\n",
    "            submitter { id }\n",
    "            principalInvestigator { name email }\n",
    "            molDBs\n",
    "            adducts\n",
    "            databases { id }\n",
    "            description\n",
    "          }\n",
    "        }\n",
    "        \"\"\",\n",
    "        {'id': ds_id}\n",
    "    )\n",
    "\n",
    "    ds = result['dataset']\n",
    "    config = json.loads(ds['configJson'])\n",
    "    metadata = json.loads(ds['metadataJson'])\n",
    "\n",
    "    # If pixel size is missing from metadata, add some values so that metadata passes validation\n",
    "    if not 'Pixel_Size' in metadata['MS_Analysis']:\n",
    "        metadata['MS_Analysis']['Pixel_Size'] = {'Xaxis': 1, 'Yaxis': 1}\n",
    "    if metadata['MS_Analysis']['Pixel_Size']['Xaxis'] < 0.1:\n",
    "        metadata['MS_Analysis']['Pixel_Size']['Xaxis'] = 100\n",
    "    if metadata['MS_Analysis']['Pixel_Size']['Yaxis'] < 0.1:\n",
    "        metadata['MS_Analysis']['Pixel_Size']['Yaxis'] = 100\n",
    "\n",
    "    # Fix double-encoding of tiptap field\n",
    "    if (ds['description'] or '').startswith('\"{\\\\\"'):\n",
    "        ds['description'] = json.loads(ds['description'])\n",
    "\n",
    "    return ds, config, metadata\n",
    "\n",
    "\n",
    "def clone_dataset(sm, ds, metadata, name, database_ids, adducts, neutral_losses, project_ids):\n",
    "    gql = sm._gqclient\n",
    "\n",
    "    result = gql.query(\n",
    "        \"\"\"\n",
    "        mutation ($input: DatasetCreateInput!) {\n",
    "          createDataset(input: $input)\n",
    "        }\n",
    "        \"\"\",\n",
    "        {\n",
    "            'input': {\n",
    "                'name': name,\n",
    "                'inputPath': ds['inputPath'],\n",
    "                'metadataJson': json.dumps(metadata),\n",
    "                'databaseIds': database_ids,\n",
    "                'adducts': adducts,\n",
    "                'neutralLosses': neutral_losses,               \n",
    "                'submitterId': gql.get_submitter_id(),\n",
    "                'groupId': gql.get_primary_group_id(),\n",
    "                'isPublic': False,\n",
    "                'description': ds['description'],\n",
    "                'projectIds': project_ids,\n",
    "                \n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    # Sleep 1 second, because METASPACE will raise an error if more than 1 dataset per second is submitted\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Return the new dataset ID\n",
    "    return json.loads(result['createDataset'])['datasetId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMInstance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"C:\\Users\\Veronica\\Documents\\LAB\\projects\\spotting\\QC\\Datasets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_of_interest = (df[\"Participant lab\"].isin(['Janfeldt/Pinto'])) \n",
    "\n",
    "datasets = df[data_of_interest][\"Dataset ID\"]\n",
    "titles = df[data_of_interest][[\"Participant lab\", \"Technology\",\"Polarity\", \"m/z range\"]].agg('_'.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-22_13h37m54s cloned to 2021-04-24_23h00m42s\n",
      "2021-03-22_13h39m15s cloned to 2021-04-24_23h00m44s\n",
      "2021-03-22_13h40m42s cloned to 2021-04-24_23h00m46s\n",
      "2021-03-22_13h49m18s cloned to 2021-04-24_23h00m48s\n",
      "2021-03-22_13h50m40s cloned to 2021-04-24_23h00m50s\n",
      "2021-03-22_13h51m44s cloned to 2021-04-24_23h00m52s\n",
      "2021-03-22_13h53m18s cloned to 2021-04-24_23h00m55s\n",
      "2021-03-22_13h59m01s cloned to 2021-04-24_23h00m57s\n",
      "2021-03-22_14h59m52s cloned to 2021-04-24_23h00m59s\n",
      "2021-03-22_15h14m29s cloned to 2021-04-24_23h01m01s\n",
      "2021-03-29_04h08m18s cloned to 2021-04-24_23h01m03s\n",
      "2021-03-29_03h23m18s cloned to 2021-04-24_23h01m06s\n",
      "2021-03-29_02h41m53s cloned to 2021-04-24_23h01m08s\n",
      "2021-03-29_02h08m19s cloned to 2021-04-24_23h01m10s\n",
      "2021-03-29_00h13m35s cloned to 2021-04-24_23h01m12s\n",
      "2021-03-28_23h24m30s cloned to 2021-04-24_23h01m14s\n",
      "2021-03-28_22h13m58s cloned to 2021-04-24_23h01m16s\n",
      "2021-03-28_21h33m31s cloned to 2021-04-24_23h01m18s\n",
      "2021-04-05_03h27m09s cloned to 2021-04-24_23h01m20s\n",
      "2021-04-05_03h27m27s cloned to 2021-04-24_23h01m22s\n",
      "2021-04-05_02h41m43s cloned to 2021-04-24_23h01m24s\n",
      "2021-04-05_02h41m20s cloned to 2021-04-24_23h01m26s\n",
      "2021-04-02_21h25m32s cloned to 2021-04-24_23h01m28s\n",
      "2021-04-02_16h49m08s cloned to 2021-04-24_23h01m30s\n",
      "2021-03-31_14h39m43s cloned to 2021-04-24_23h01m32s\n",
      "2021-03-31_01h23m36s cloned to 2021-04-24_23h01m34s\n",
      "2021-04-13_14h54m56s cloned to 2021-04-24_23h01m36s\n",
      "2021-04-13_14h55m01s cloned to 2021-04-24_23h01m38s\n",
      "2021-04-13_14h54m11s cloned to 2021-04-24_23h01m40s\n",
      "2021-04-13_14h54m48s cloned to 2021-04-24_23h01m42s\n",
      "2021-04-13_14h54m43s cloned to 2021-04-24_23h01m44s\n",
      "2021-04-13_14h54m24s cloned to 2021-04-24_23h01m46s\n",
      "2021-04-13_14h54m03s cloned to 2021-04-24_23h01m48s\n",
      "2021-04-13_14h54m02s cloned to 2021-04-24_23h01m50s\n",
      "2021-04-12_12h13m45s cloned to 2021-04-24_23h01m52s\n",
      "2021-04-12_12h13m43s cloned to 2021-04-24_23h01m54s\n",
      "2021-04-12_12h13m38s cloned to 2021-04-24_23h01m56s\n",
      "2021-04-12_12h13m41s cloned to 2021-04-24_23h01m58s\n",
      "2021-04-12_11h46m58s cloned to 2021-04-24_23h02m00s\n",
      "2021-04-12_11h41m13s cloned to 2021-04-24_23h02m02s\n",
      "2021-04-12_11h47m01s cloned to 2021-04-24_23h02m04s\n",
      "2021-04-12_11h41m52s cloned to 2021-04-24_23h02m06s\n",
      "2021-03-22_11h53m50s cloned to 2021-04-24_23h02m08s\n",
      "2021-03-22_11h57m41s cloned to 2021-04-24_23h02m10s\n",
      "2021-03-22_11h56m42s cloned to 2021-04-24_23h02m12s\n",
      "2021-03-22_11h55m58s cloned to 2021-04-24_23h02m14s\n",
      "2021-03-22_11h50m09s cloned to 2021-04-24_23h02m16s\n",
      "2021-03-22_11h54m33s cloned to 2021-04-24_23h02m18s\n",
      "2021-03-08_09h47m44s cloned to 2021-04-24_23h02m20s\n",
      "2021-03-08_09h34m01s cloned to 2021-04-24_23h02m22s\n"
     ]
    }
   ],
   "source": [
    "ds_ids = [\n",
    "    '2021-02-15_21h23m26s',\n",
    "]\n",
    "\n",
    "new_ds_ids = []\n",
    "\n",
    "for ds_id in ds_ids:\n",
    "\n",
    "    # Get the existing dataset data\n",
    "    ds, config, metadata = get_raw_dataset(sm, ds_id)\n",
    "    is_positive_mode = metadata['MS_Analysis']['Polarity'] == 'Positive'\n",
    "\n",
    "    Adjust other parameters\n",
    "    database_ids = [db['id'] for db in ds['databases']]\n",
    "    # Append to database_ids to add more databases, e.g.\n",
    "    if 304 not in database_ids:\n",
    "        database_ids.append(304)\n",
    "\n",
    "    # Append to adducts if you want to add adducts that aren't available through the UI\n",
    "    adducts = ds['adducts']\n",
    "    neutral_losses = ds['neutral_losses']\n",
    "\n",
    "    new_ds_id = clone_dataset(\n",
    "        sm=sm,\n",
    "        ds=ds,\n",
    "        metadata=metadata,\n",
    "        name=ds['name'], # Rename or add a suffix if desired\n",
    "        database_ids=database_ids,\n",
    "        adducts=adducts,\n",
    "        neutral_losses = neutral_losses,\n",
    "        # If uploading in bulk, it's a good idea to put things in projects. Create a project through\n",
    "        # the web and copy the ID from the url here:\n",
    "        project_ids=['62d1990a-a4ff-11eb-96db-abcc9848804b'],\n",
    "    )\n",
    "    new_ds_ids.append(new_ds_id)\n",
    "\n",
    "    print(f'{ds_id} cloned to {new_ds_id}')\n",
    "    df.loc[df['Dataset ID'] == ds_id, 'Clone ID'] = new_ds_id\n",
    "\n",
    "df.to_csv(r\"C:\\Users\\Veronica\\Documents\\LAB\\projects\\spotting\\QC\\Datasets_modified.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metabolite_analysis]",
   "language": "python",
   "name": "conda-env-metabolite_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
