{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate classification models & dump images for diagnosis/labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:06:33.610129Z",
     "start_time": "2021-06-02T16:06:33.498927Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:06:34.264180Z",
     "start_time": "2021-06-02T16:06:33.940237Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from scipy.ndimage import binary_dilation\n",
    "from sklearn import clone\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, cross_validate\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from definitions import ROOT_DIR\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from metaspace.sm_annotation_utils import SMInstance\n",
    "from metaspace.image_processing import clip_hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:06:34.285960Z",
     "start_time": "2021-06-02T16:06:34.265231Z"
    }
   },
   "outputs": [],
   "source": [
    "# Suppress warnings, because many models spam them during feature selection\n",
    "# as some subsets of features just don't have enough information to make\n",
    "# a good model.\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:06:34.833722Z",
     "start_time": "2021-06-02T16:06:34.810682Z"
    }
   },
   "outputs": [],
   "source": [
    "def colorize_image_with_mask(image, mask):\n",
    "    \"\"\"Plotting function for combining a colorized ion image with a spot mask\"\"\"\n",
    "    image = clip_hotspots(image)\n",
    "    on_spot_colorized = plt.cm.cividis(image)\n",
    "    off_spot_colorized = plt.cm.magma(image)\n",
    "    return np.where(mask[:,:,np.newaxis], on_spot_colorized, off_spot_colorized)\n",
    "    \n",
    "def save_image_with_mask(image, mask, fname):\n",
    "    plt.imsave(fname, colorize_image_with_mask(image, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:06:35.191813Z",
     "start_time": "2021-06-02T16:06:35.170946Z"
    }
   },
   "outputs": [],
   "source": [
    "def crop_zeros(img):\n",
    "    \"\"\"Crop an image, removing all empty outer rows/columns\"\"\"\n",
    "    cols = np.flatnonzero(np.count_nonzero(img, axis=0) != 0)\n",
    "    rows = np.flatnonzero(np.count_nonzero(img, axis=1) != 0)\n",
    "    top = rows[0]\n",
    "    bottom = rows[-1] + 1\n",
    "    left = cols[0]\n",
    "    right = cols[-1] + 1\n",
    "\n",
    "    return img[top:bottom, left:right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:06:35.605701Z",
     "start_time": "2021-06-02T16:06:35.584102Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_mispredictions(model, X, y):\n",
    "    \"\"\"\n",
    "    Find which values would be mispredicted, returning two lists:\n",
    "        * indexes of items that would be falsely predicted as positives\n",
    "        * indexes of items that would be falsely predicted as negatives\n",
    "        \n",
    "    cross_val_predict uses a shuffled 5-fold test-train split so that each chunk of \n",
    "    20% of the input data gets its own model that was trained on the other 80%, \n",
    "    ensuring that the items being predicted aren't included in the training data.\n",
    "    \"\"\"\n",
    "    preds = cross_val_predict(model, X, y)\n",
    "    mispreds = preds != y\n",
    "    fpos_idxs = np.flatnonzero(mispreds & ~y)\n",
    "    fneg_idxs = np.flatnonzero(mispreds & y)\n",
    "        \n",
    "    return fpos_idxs, fneg_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:06:36.849373Z",
     "start_time": "2021-06-02T16:06:36.826914Z"
    }
   },
   "outputs": [],
   "source": [
    "p_root_dir = Path(ROOT_DIR)\n",
    "p_analysis = p_root_dir  / \"4_model_evaluation\"\n",
    "\n",
    "p_grids = p_analysis / r\"data_for_model_training\\labelled_set_masks\\grid_masks\"\n",
    "p_labelled_set = p_analysis / r\"data_for_model_training\\labelled_set\"\n",
    "\n",
    "p_wellmap = p_root_dir / \"5_data_analysis/wellmap.csv\"\n",
    "\n",
    "# Paths for evaluation\n",
    "p_eval = p_analysis/ \"model_evaluation\"\n",
    "p_eval_preds = p_eval / 'predictions.csv'\n",
    "# False positives/negatives - preview output from model prediction for molecules with known labels\n",
    "# Note that all files in these directories are cleared before a prediction run\n",
    "p_eval_fpos = p_eval / 'false_positives'\n",
    "p_eval_fneg = p_eval / 'false_negatives'\n",
    "p_eval_tpos = p_eval / 'true_positives'\n",
    "p_eval_tneg = p_eval / 'true_negatives'\n",
    "# Unknown positives/negatives - preview output from model prediction for molecules with no label\n",
    "# Note that all files in these directories are cleared before a prediction run\n",
    "p_eval_upos = p_eval / 'unknown_positives'\n",
    "p_eval_uneg = p_eval / 'unknown_negatives'\n",
    "# Manually labeled positives/negatives - Move preview files from any of the above directories into \n",
    "# these directories to add to the labelled data. Make sure to re-run the appropriate steps \n",
    "# in \"Input data\" to detect the changes\n",
    "p_eval_lpos = p_eval / 'manual_label_positives'\n",
    "p_eval_lneg = p_eval / 'manual_label_negatives'\n",
    "\n",
    "# Directories for three-state positive/unsure/negative classification\n",
    "p_tri_pos = p_eval / 'three-state' / 'positive'\n",
    "p_tri_unk = p_eval / 'three-state' / 'unsure'\n",
    "p_tri_neg = p_eval / 'three-state' / 'negative'\n",
    "\n",
    "\n",
    "# METASPACE\n",
    "database = ('Spotting_project_compounds-v9', 'feb2021')\n",
    "fdr = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key: ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "from metaspace import SMInstance\n",
    "\n",
    "sm = SMInstance(host='https://metaspace2020.eu')\n",
    "\n",
    "if not sm.logged_in():\n",
    "    # Using getpass here prevents the API key from being accidentally saved with this notebook.\n",
    "    api_key = getpass.getpass(prompt='API key: ', stream=None)\n",
    "    sm.login(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:06:38.160316Z",
     "start_time": "2021-06-02T16:06:38.112047Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get dataset IDs based on Quality_Labels.csv files \n",
    "dataset_ids = pd.concat([\n",
    "    pd.read_csv(f)\n",
    "    for f in p_labelled_set.rglob(\"*Quality_Labels.csv\")\n",
    "]).dataset_id.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:08:50.018554Z",
     "start_time": "2021-06-02T16:06:38.808059Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for 2021-02-15_17h57m16s (0/14)\n",
      "Downloading images for 2021-02-16_06h28m34s (1/14)\n",
      "Downloading images for 2021-02-17_18h14m40s (2/14)\n",
      "Downloading images for 2021-02-19_12h11m04s (3/14)\n",
      "Downloading images for 2021-02-28_07h58m29s (4/14)\n",
      "Downloading images for 2021-03-05_15h01m51s (5/14)\n",
      "Downloading images for 2021-03-07_11h36m10s (6/14)\n",
      "Downloading images for 2021-03-08_10h38m53s (7/14)\n",
      "Downloading images for 2021-03-24_21h05m49s (8/14)\n",
      "Downloading images for 2021-03-24_23h18m35s (9/14)\n",
      "Downloading images for 2021-04-11_21h47m45s (10/14)\n",
      "Downloading images for 2021-04-11_21h56m24s (11/14)\n",
      "Downloading images for 2021-04-26_15h41m55s (12/14)\n",
      "Downloading images for 2021-04-26_16h00m28s (13/14)\n"
     ]
    }
   ],
   "source": [
    "# Images from METASPACE\n",
    "# NOTE: Hotspot clipping is applied at this step, so `np.max(image)` \n",
    "# represents the 99th percentile intensity for the rest of the script.\n",
    "#\n",
    "# Ignore any warnings about connection pools in this step\n",
    "\n",
    "images = []\n",
    "for i, ds_id in enumerate(dataset_ids):\n",
    "    print(f'Downloading images for {ds_id} ({i}/{len(dataset_ids)})')\n",
    "    for img in sm.dataset(id=ds_id).all_annotation_images(\n",
    "        fdr=fdr, \n",
    "        database=database, \n",
    "        only_first_isotope=True, \n",
    "        scale_intensity=True, \n",
    "        hotspot_clipping=False,\n",
    "    ):\n",
    "        # Exclude annotations with no first-isotopic-image\n",
    "        if img[0] is not None:\n",
    "            images.append({\n",
    "                'dataset_id': ds_id,\n",
    "                'formula': img.formula,\n",
    "                'adduct': img.adduct,\n",
    "                'neutral_loss': img.neutral_loss or '',\n",
    "                'image': img[0],\n",
    "                'filename': f'{ds_id}_{img.formula}_{img.adduct}_{img.neutral_loss}.png'.replace('+', ''),\n",
    "            })\n",
    "            \n",
    "images_df = pd.DataFrame(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:09:00.872379Z",
     "start_time": "2021-06-02T16:09:00.846905Z"
    }
   },
   "outputs": [],
   "source": [
    "# Wellmap and grids\n",
    "wellmap = pd.read_csv(p_wellmap)\n",
    "grids = {\n",
    "    ds_id: np.load(p_grids / f'{ds_id}.npy') \n",
    "    for ds_id in dataset_ids\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:09:01.342997Z",
     "start_time": "2021-06-02T16:09:01.288020Z"
    }
   },
   "outputs": [],
   "source": [
    "# Image labels from Quality_Labels.csv files\n",
    "labeled_anns = []\n",
    "for i in p_labelled_set.rglob(\"*Quality_Labels.csv\"):\n",
    "    data = pd.read_csv(i)\n",
    "    data = data.loc[:, ['dataset_id', 'formula', 'adduct', 'neutral_loss', 'score']]\n",
    "    data.neutral_loss.fillna('', inplace=True)\n",
    "    labeled_anns.append(data)\n",
    "\n",
    "labeled_anns_df = pd.concat(labeled_anns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import image labels from the manual_label directories\n",
    "\n",
    "If you use these directories for labelling, re-run every cell from this point onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:09:02.819503Z",
     "start_time": "2021-06-02T16:09:02.794776Z"
    }
   },
   "outputs": [],
   "source": [
    "# Image labels from the \"manual_label\" directories\n",
    "manual_labels = []\n",
    "for score, labels_path in [(1, p_eval_lpos), (0, p_eval_lneg)]:\n",
    "    labels_path.mkdir(parents=True, exist_ok=True)\n",
    "    for f in labels_path.glob('*.png'):\n",
    "        manual_labels.append({\n",
    "            'filename': f.name,\n",
    "            'manual_score': score,\n",
    "        })\n",
    "if manual_labels:\n",
    "    manual_labels_df = pd.DataFrame(manual_labels)\n",
    "else:\n",
    "    manual_labels_df = pd.DataFrame({'filename': pd.Series(dtype=str), 'manual_score': pd.Series(dtype='i')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:09:03.532632Z",
     "start_time": "2021-06-02T16:09:03.498051Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine them for easier access\n",
    "merged_df = (\n",
    "    images_df\n",
    "    # Add `how=left` when merging with wellmap to include non-spotted formulas\n",
    "    .merge(wellmap[['well', 'formula', 'name_short']], on=['formula'])\n",
    "    .merge(labeled_anns_df, on=['dataset_id', 'formula', 'adduct', 'neutral_loss'], how='left')\n",
    "    .merge(manual_labels_df, on='filename', how='left')\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Merge the \"manual_score\" column into \"score\"\n",
    "merged_df['score'] = np.where(merged_df.score.isna(), merged_df.manual_score, merged_df.score)\n",
    "del merged_df['manual_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:09:13.899106Z",
     "start_time": "2021-06-02T16:09:05.030910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spot_occupancy</th>\n",
       "      <th>spot_occupancy_thresholded</th>\n",
       "      <th>far_bg_occupancy</th>\n",
       "      <th>occupancy_vs_far_bg_ratio</th>\n",
       "      <th>in_n_spots</th>\n",
       "      <th>spot_intensity</th>\n",
       "      <th>intensity_vs_other_spots_ratio</th>\n",
       "      <th>intensity_vs_far_bg_ratio</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288462</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.846154</td>\n",
       "      <td>2</td>\n",
       "      <td>16360.128906</td>\n",
       "      <td>13205.733809</td>\n",
       "      <td>1.636013e+07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.288462</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.846154</td>\n",
       "      <td>4</td>\n",
       "      <td>1543.650146</td>\n",
       "      <td>10562.654334</td>\n",
       "      <td>1.543650e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>5.749220</td>\n",
       "      <td>1</td>\n",
       "      <td>43.455296</td>\n",
       "      <td>43455.295563</td>\n",
       "      <td>9.790880e+03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>85.875339</td>\n",
       "      <td>2</td>\n",
       "      <td>28936.888672</td>\n",
       "      <td>13629.185383</td>\n",
       "      <td>5.237944e+05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>79.386183</td>\n",
       "      <td>4</td>\n",
       "      <td>1471.808350</td>\n",
       "      <td>8104.936776</td>\n",
       "      <td>8.299503e+04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spot_occupancy  spot_occupancy_thresholded  far_bg_occupancy  \\\n",
       "0        0.288462                    0.230769          0.000000   \n",
       "1        0.288462                    0.269231          0.000000   \n",
       "2        0.057692                    0.057692          0.000035   \n",
       "3        0.864865                    0.810811          0.000071   \n",
       "4        0.810811                    0.810811          0.000214   \n",
       "\n",
       "   occupancy_vs_far_bg_ratio  in_n_spots  spot_intensity  \\\n",
       "0                  28.846154           2    16360.128906   \n",
       "1                  28.846154           4     1543.650146   \n",
       "2                   5.749220           1       43.455296   \n",
       "3                  85.875339           2    28936.888672   \n",
       "4                  79.386183           4     1471.808350   \n",
       "\n",
       "   intensity_vs_other_spots_ratio  intensity_vs_far_bg_ratio  score  \n",
       "0                    13205.733809               1.636013e+07    NaN  \n",
       "1                    10562.654334               1.543650e+06    NaN  \n",
       "2                    43455.295563               9.790880e+03    NaN  \n",
       "3                    13629.185383               5.237944e+05    NaN  \n",
       "4                     8104.936776               8.299503e+04    NaN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_far_bg(mask, bg):\n",
    "    \"\"\"Gets mask for background pixels that are at least 4 radii away from the spot\"\"\"\n",
    "    # 3 iterations = (1+3=)4x the spot radius\n",
    "    expanded_spot = binary_dilation(mask, crop_zeros(mask), iterations=3)\n",
    "    return bg & ~expanded_spot\n",
    "\n",
    "def occ(px):\n",
    "    \"\"\"Calculates non-zero % of the given array\"\"\"\n",
    "    return np.count_nonzero(px) / px.size\n",
    "\n",
    "\n",
    "metrics = []\n",
    "for row in merged_df.itertuples():\n",
    "    grid = grids[row.dataset_id]\n",
    "    \n",
    "    mask = grid == row.well\n",
    "    bg = grid == 0\n",
    "    far_bg = calc_far_bg(mask, bg)\n",
    "        \n",
    "    in_mask = row.image[mask]\n",
    "    in_bg = row.image[bg]\n",
    "    in_far_bg = row.image[far_bg]\n",
    "    in_other_spots = row.image[~bg & ~mask]\n",
    "    \n",
    "    # Calculate threshold (0.01 * 99th percentile) \n",
    "    # (note the image is already hotspot-removed, so the max is the 99th percentile)\n",
    "    threshold = np.max(row.image) * 0.01\n",
    "\n",
    "    metrics.append({\n",
    "        # Original metrics\n",
    "        # NOTE: The constant in the denominator of `on_off_ratio` was changed to\n",
    "        # 0.001 as it seemed to produce slightly better results\n",
    "#         'occupancy_ratio': (occ(in_mask) * 100) / (occ(in_bg) * 100 + 1),\n",
    "#         'on_off_ratio': (np.mean(in_mask)) / (np.mean(in_bg) + 0.001),\n",
    "        \n",
    "        # Single-spot occupancy %\n",
    "        'spot_occupancy': occ(in_mask),\n",
    "        'spot_occupancy_thresholded': occ(in_mask > threshold),\n",
    "        # Other occupancy metrics\n",
    "#         'image_occupancy': occ(row.image),\n",
    "#         'other_spots_occupancy': occ(in_other_spots),\n",
    "#         'bg_occupancy': occ(in_bg),\n",
    "        'far_bg_occupancy': occ(in_far_bg),\n",
    "        'occupancy_vs_far_bg_ratio' : (occ(in_mask) * 100) / (occ(in_far_bg) * 100 + 1),\n",
    "        \n",
    "        # How many spots have a non-zero pixel\n",
    "        'in_n_spots': len(np.unique(grid[(grid != 0) & (row.image > threshold)])),\n",
    "        \n",
    "        # Intensity ratios\n",
    "        'spot_intensity' : np.mean(in_mask),\n",
    "        'intensity_vs_other_spots_ratio': np.mean(in_mask) / (np.mean(in_other_spots) + 0.001),\n",
    "        'intensity_vs_far_bg_ratio': np.mean(in_mask) / (np.mean(in_far_bg) + 0.001),\n",
    "        \n",
    "        'score': row.score,\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, index=merged_df.index)\n",
    "metrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate models\n",
    "\n",
    "This section uses the calculated metrics and labeled data to train a set of models \n",
    "and find which features are best for predicting the labels. \n",
    "It uses two strategies for evaluation:\n",
    "\n",
    "* Hold-out validation - this splits the labeled data into 80% for training, 20% for testing\n",
    "* Cross-Validation - this uses the full labeled data, but trains 5 different models, each\n",
    "    with a different combinations of inputs in the 80% training set, so that each input \n",
    "    can be tested by a model that didn't use that input as part of the training.\n",
    "    This approach reports a much more numerically stable accuracy value it can use \n",
    "    the full input set for evaluation.\n",
    "    However, it shouldn't be used for fine-tuning the model hyperparameters \n",
    "    (the input variables when constructing the model), as this can lead to overfitting.\n",
    "    \n",
    "   \n",
    "The output is a DataFrame `eval_results_df` that shows for each model/# of features:\n",
    "* Which combination of features worked best\n",
    "* The accuracy/F1 scores\n",
    "* The # of false positives & false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:09:16.020437Z",
     "start_time": "2021-06-02T16:09:15.995812Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare input data\n",
    "input_df = metrics_df[~metrics_df.score.isna()]  # Exclude unlabeled rows\n",
    "input_df = input_df.sample(frac=1.0)  # Shuffle rows\n",
    "X = input_df.drop(columns=['score'])\n",
    "y = input_df.score.astype('i').values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:14:59.114845Z",
     "start_time": "2021-06-02T16:09:16.668321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<catboost.core.CatBoostClassifier object at 0x0000019ABCC0A040> 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>n_features</th>\n",
       "      <th>features</th>\n",
       "      <th>cv_accuracy</th>\n",
       "      <th>cv_f1</th>\n",
       "      <th>holdout_accuracy</th>\n",
       "      <th>holdout_f1</th>\n",
       "      <th>n_fpos</th>\n",
       "      <th>n_fneg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n",
       "      <td>4</td>\n",
       "      <td>spot_occupancy, spot_occupancy_thresholded, oc...</td>\n",
       "      <td>0.879376</td>\n",
       "      <td>0.751756</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  n_features  \\\n",
       "0  <catboost.core.CatBoostClassifier object at 0x...           4   \n",
       "\n",
       "                                            features  cv_accuracy     cv_f1  \\\n",
       "0  spot_occupancy, spot_occupancy_thresholded, oc...     0.879376  0.751756   \n",
       "\n",
       "   holdout_accuracy  holdout_f1  n_fpos  n_fneg  \n",
       "0          0.774194    0.363636       9      10  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Models to try\n",
    "models_to_eval = [\n",
    "    CatBoostClassifier(verbose=False),\n",
    "#     LinearSVC(class_weight='balanced'),\n",
    "#     DecisionTreeClassifier(max_depth=3),\n",
    "#     BaggingClassifier(LinearSVC(), n_estimators=3, bootstrap_features=True),\n",
    "]\n",
    "max_features_to_consider = 4\n",
    "\n",
    "eval_results = []\n",
    "\n",
    "for model in models_to_eval:\n",
    "    model_name = str(model)\n",
    "    for n_features in range(4, max_features_to_consider + 1):\n",
    "        print(model_name, n_features)\n",
    "        # SequentialFeatureSelector finds the set of N features that give the best scores\n",
    "        sfs = SequentialFeatureSelector(model, n_features_to_select=n_features, n_jobs=-1, cv=RepeatedStratifiedKFold())\n",
    "        sfs.fit(X_train, y_train)\n",
    "        best_features = X.columns[sfs.support_]\n",
    "        \n",
    "        # Evaluate using cross-validation\n",
    "        X_subset = X[best_features].values\n",
    "        fpos_idxs, fneg_idxs = get_mispredictions(model, X_subset, y)\n",
    "        # Use a repeating cross-validator so that results are averaged over ~50 runs\n",
    "        cv = RepeatedStratifiedKFold()\n",
    "        cv_scores = cross_validate(model, X_subset, y, cv=cv, scoring=['accuracy','f1'])\n",
    "        cv_accuracy = np.mean(cv_scores['test_accuracy'])\n",
    "        cv_f1 = np.mean(cv_scores['test_f1'])\n",
    "        \n",
    "        # Evaluate using hold-out validation\n",
    "        trained_subset_model = clone(model).fit(X_train[best_features].values, y_train)\n",
    "        holdout_accuracy = trained_subset_model.score(X_test[best_features].values, y_test)\n",
    "        holdout_f1 = f1_score(y_test, trained_subset_model.predict(X_test[best_features].values))\n",
    "        \n",
    "        eval_results.append({\n",
    "            'model': model_name,\n",
    "            'n_features': n_features,\n",
    "            'features': ', '.join(best_features),\n",
    "            'cv_accuracy': cv_accuracy,\n",
    "            'cv_f1': cv_f1,\n",
    "            'holdout_accuracy': holdout_accuracy,\n",
    "            'holdout_f1': holdout_f1,\n",
    "            'n_fpos': len(fpos_idxs),\n",
    "            'n_fneg': len(fneg_idxs),\n",
    "            # Uncomment to include the idxs of false positives/negatives to see which\n",
    "            # inputs are repeatedly mispredicted regardless of the model\n",
    "            # 'fpos_idxs': fpos_idxs,\n",
    "            # 'fneg_idxs': fneg_idxs,\n",
    "        })\n",
    "        \n",
    "eval_results_df = pd.DataFrame(eval_results)\n",
    "\n",
    "eval_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spot_occupancy, spot_occupancy_thresholded, occupancy_vs_far_bg_ratio, intensity_vs_other_spots_ratio\n"
     ]
    }
   ],
   "source": [
    "print(eval_results_df.features.iloc[0])\n",
    "# print(eval_results_df.features.iloc[1])\n",
    "# print(eval_results_df.features.iloc[2])\n",
    "# print(eval_results_df.features.iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:22:29.917816Z",
     "start_time": "2021-06-02T16:22:29.830719Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show behavior of accuracy as number of features increases\n",
    "# sns.lineplot(data=eval_results_df, x='n_features', y='cv_accuracy', hue='model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine results for a specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:54:58.014519Z",
     "start_time": "2021-06-02T16:54:57.991621Z"
    }
   },
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(verbose=False)\n",
    "features = ['spot_occupancy_thresholded', 'occupancy_vs_far_bg_ratio', 'intensity_vs_far_bg_ratio', 'intensity_vs_other_spots_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Train a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:58:29.785376Z",
     "start_time": "2021-06-02T16:58:26.677693Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model on labeled data\n",
    "train_df = metrics_df[~metrics_df.score.isna()]  # Exclude unlabeled rows\n",
    "train_df = input_df.sample(frac=1.0)  # Shuffle rows\n",
    "X_df = train_df.drop(columns=['score'])[features]\n",
    "y = train_df.score.astype('i').values\n",
    "trained_model = clone(model).fit(X_df.values, y)\n",
    "\n",
    "# Make predictions for unlabeled data\n",
    "unlabeled_df = metrics_df[metrics_df.score.isna()][features]\n",
    "unlabeled_predictions_df = pd.DataFrame({\n",
    "    'pred_val': trained_model.predict_proba(unlabeled_df.values)[:, 1]\n",
    "}, index=unlabeled_df.index)\n",
    "\n",
    "# Make cross-validated predictions for labeled data\n",
    "labeled_predictions_df = pd.DataFrame({\n",
    "    'pred_val': cross_val_predict(model, X_df.values, y, method='predict_proba')[:, 1]\n",
    "}, index=X_df.index)\n",
    "\n",
    "# Combine predictions\n",
    "predictions_df = pd.concat([unlabeled_predictions_df, labeled_predictions_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Both options: Assign labels to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:58:47.130742Z",
     "start_time": "2021-06-02T16:58:47.104805Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make combined DF\n",
    "output_df = merged_df.drop(columns='score').join(metrics_df).join(predictions_df)\n",
    "\n",
    "# Add two-state and three-state classes\n",
    "output_df['pred_twostate'] = np.where(output_df.pred_val < 0.5, 0, 1)\n",
    "unsure_range = [0.2, 0.8] # Lowest & highest values to include in the \"unsure\" class\n",
    "# This assigns 0 = negative, 1 = unsure, 2 = positive\n",
    "output_df['pred_threestate'] = np.digitize(output_df.pred_val, unsure_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write predictions CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T16:58:48.369446Z",
     "start_time": "2021-06-02T16:58:48.294036Z"
    }
   },
   "outputs": [],
   "source": [
    "csv_df = output_df.drop(columns=['image', 'filename']) # Skip unwanted columns\n",
    "\n",
    "for dataset_id, results_df in csv_df.groupby('dataset_id'):\n",
    "    output_path = p_eval / f'{dataset_id}_predictions.csv'\n",
    "    results_df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write image files into false positives, false negatives, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-02T16:58:53.453Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clean output directories\n",
    "for output_path in [\n",
    "    p_eval_fpos, p_eval_fneg, p_eval_tpos, p_eval_tneg, p_eval_upos, p_eval_uneg, \n",
    "    p_tri_pos, p_tri_unk, p_tri_neg\n",
    "]:\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    for f in output_path.glob('*.png'):\n",
    "        f.unlink()  # Delete existing files\n",
    "\n",
    "# Write images with two-state classification\n",
    "for row in output_df.itertuples():\n",
    "    mask = grids[row.dataset_id] == row.well\n",
    "    \n",
    "    # Figure out which directory to use\n",
    "    if row.score == 0:\n",
    "        twostate_path = [p_eval_tneg, p_eval_fpos][row.pred_twostate]\n",
    "    elif row.score == 1:\n",
    "        twostate_path = [p_eval_fneg, p_eval_tpos][row.pred_twostate]\n",
    "    else:\n",
    "        twostate_path = [p_eval_uneg, p_eval_upos][row.pred_twostate]\n",
    "    \n",
    "    save_image_with_mask(row.image, mask, twostate_path / row.filename)\n",
    "    \n",
    "# Write images with three-state classification\n",
    "for row in output_df.itertuples():\n",
    "    mask = grids[row.dataset_id] == row.well\n",
    "    \n",
    "    threestate_path = [p_tri_neg, p_tri_unk, p_tri_pos][row.pred_threestate]\n",
    "    \n",
    "    save_image_with_mask(row.image, mask, threestate_path / row.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save trained model\n",
    "\n",
    "Note: This JSON export only works for CatBoost. \n",
    "scikit-learn models don't have a standardized export format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-02T16:59:12.221Z"
    }
   },
   "outputs": [],
   "source": [
    "trained_model.save_model(p_eval / 'model.json', format='json', pool=Pool(X_df.values, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Load an existing model\n",
    "Uses a saved model from the last step of this file\n",
    "\n",
    "NOTE: This approach doesn't use cross-validated predictions for the labelled training data,\n",
    "so it shouldn't be used for analyzing the model or refining the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_root_dir = Path(ROOT_DIR)\n",
    "p_analysis = p_root_dir  / \"4_model_evaluation\"\n",
    "\n",
    "p_grids = p_root_dir / r\"2_grid_calibration\\grid_masks\"\n",
    "p_wellmap = p_root_dir / \"5_data_analysis/wellmap.csv\"\n",
    "p_model = p_root_dir / r\"4_model_evaluation/model_evaluation/model.json\"\n",
    "\n",
    "# Paths for evaluation\n",
    "p_eval = p_analysis/ \"model_application\"\n",
    "p_eval_preds = p_eval / 'predictions.csv'\n",
    "# False positives/negatives - preview output from model prediction for molecules with known labels\n",
    "# Note that all files in these directories are cleared before a prediction run\n",
    "p_eval_fpos = p_eval / 'false_positives'\n",
    "p_eval_fneg = p_eval / 'false_negatives'\n",
    "p_eval_tpos = p_eval / 'true_positives'\n",
    "p_eval_tneg = p_eval / 'true_negatives'\n",
    "# Unknown positives/negatives - preview output from model prediction for molecules with no label\n",
    "# Note that all files in these directories are cleared before a prediction run\n",
    "p_eval_upos = p_eval / 'unknown_positives'\n",
    "p_eval_uneg = p_eval / 'unknown_negatives'\n",
    "# Manually labeled positives/negatives - Move preview files from any of the above directories into \n",
    "# these directories to add to the labelled data. Make sure to re-run the appropriate steps \n",
    "# in \"Input data\" to detect the changes\n",
    "p_eval_lpos = p_eval / 'manual_label_positives'\n",
    "p_eval_lneg = p_eval / 'manual_label_negatives'\n",
    "\n",
    "# Directories for three-state positive/unsure/negative classification\n",
    "p_tri_pos = p_eval / 'three-state' / 'positive'\n",
    "p_tri_unk = p_eval / 'three-state' / 'unsure'\n",
    "p_tri_neg = p_eval / 'three-state' / 'negative'\n",
    "\n",
    "\n",
    "# METASPACE\n",
    "database = ('Spotting_project_compounds-v9', 'feb2021')\n",
    "fdr = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key: ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "from metaspace import SMInstance\n",
    "\n",
    "sm = SMInstance(host='https://metaspace2020.eu')\n",
    "\n",
    "if not sm.logged_in():\n",
    "    # Using getpass here prevents the API key from being accidentally saved with this notebook.\n",
    "    api_key = getpass.getpass(prompt='API key: ', stream=None)\n",
    "    sm.login(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ids = [x.stem[-20:] for x in p_grids.rglob(\"*.npy\")]\n",
    "dataset_names = [x.stem for x in p_grids.rglob(\"*.npy\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images for 2021-06-18_10h37m54s (0/22)\n",
      "Downloading images for 2021-06-21_12h29m16s (1/22)\n",
      "Downloading images for 2021-06-18_10h41m59s (2/22)\n",
      "Downloading images for 2021-06-21_12h32m53s (3/22)\n",
      "Downloading images for 2021-06-18_10h45m03s (4/22)\n",
      "Downloading images for 2021-06-21_12h34m54s (5/22)\n",
      "Downloading images for 2021-06-18_10h49m47s (6/22)\n",
      "Downloading images for 2021-06-21_12h38m55s (7/22)\n",
      "Downloading images for 2021-06-18_10h52m02s (8/22)\n",
      "Downloading images for 2021-06-21_12h41m08s (9/22)\n",
      "Downloading images for 2021-06-18_10h54m38s (10/22)\n",
      "Downloading images for 2021-06-21_12h45m04s (11/22)\n",
      "Downloading images for 2021-06-18_10h58m25s (12/22)\n",
      "Downloading images for 2021-06-21_12h48m21s (13/22)\n",
      "Downloading images for 2021-06-18_10h59m48s (14/22)\n",
      "Downloading images for 2021-06-21_12h50m44s (15/22)\n",
      "Downloading images for 2021-06-18_11h04m19s (16/22)\n",
      "Downloading images for 2021-06-21_12h54m04s (17/22)\n",
      "Downloading images for 2021-06-21_15h10m30s (18/22)\n",
      "Downloading images for 2021-06-23_23h19m02s (19/22)\n",
      "Downloading images for 2021-06-18_11h09m13s (20/22)\n",
      "Downloading images for 2021-06-21_12h59m59s (21/22)\n"
     ]
    }
   ],
   "source": [
    "# Images from METASPACE\n",
    "# NOTE: Hotspot clipping is applied at this step, so `np.max(image)` \n",
    "# represents the 99th percentile intensity for the rest of the script.\n",
    "#\n",
    "# Ignore any warnings about connection pools in this step\n",
    "\n",
    "images = []\n",
    "results = []\n",
    "for i, ds_id in enumerate(dataset_ids):\n",
    "    print(f'Downloading images for {ds_id} ({i}/{len(dataset_ids)})')\n",
    "    \n",
    "    results = sm.dataset(id=ds_id).results(database=('Spotting_project_compounds-v9', 'feb2021'), include_neutral_losses=True)\n",
    "    for img in sm.dataset(id=ds_id).all_annotation_images(\n",
    "        fdr=fdr, \n",
    "        database=database, \n",
    "        only_first_isotope=True, \n",
    "        scale_intensity=True, \n",
    "        hotspot_clipping=False\n",
    "    ):\n",
    "        # Exclude annotations with no first-isotopic-image\n",
    "        if img[0] is not None:\n",
    "            images.append({\n",
    "                'dataset_id': ds_id,\n",
    "                'dataset_name': dataset_names[i],\n",
    "                'formula': img.formula,\n",
    "                'adduct': img.adduct,\n",
    "                'neutral_loss': img.neutral_loss or '',\n",
    "                'image': img[0],\n",
    "                'filename': f'{ds_id}_{img.formula}_{img.adduct}_{img.neutral_loss}.png'.replace('+', ''),\n",
    "                'msm' : results.loc[(img.formula, img.adduct, img.neutral_loss), 'msm']\n",
    "            })\n",
    "            \n",
    "images_df = pd.DataFrame(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wellmap = pd.read_csv(p_wellmap)\n",
    "grids = {\n",
    "    ds_name: np.load(p_grids / f'{ds_name}.npy') \n",
    "    for ds_name in dataset_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine them for easier access\n",
    "merged_df = (\n",
    "    images_df\n",
    "    # Add `how=left` when merging with wellmap to include non-spotted formulas\n",
    "    .merge(wellmap[['well', 'formula', 'name_short']], on=['formula'])\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>on_off_ratio</th>\n",
       "      <th>occupancy_ratio</th>\n",
       "      <th>spot_occupancy</th>\n",
       "      <th>spot_occupancy_thresholded</th>\n",
       "      <th>image_occupancy</th>\n",
       "      <th>other_spots_occupancy</th>\n",
       "      <th>bg_occupancy</th>\n",
       "      <th>far_bg_occupancy</th>\n",
       "      <th>occupancy_vs_far_bg_ratio</th>\n",
       "      <th>in_n_spots</th>\n",
       "      <th>spot_intensity</th>\n",
       "      <th>spot_intensity_bgr_corrected</th>\n",
       "      <th>spot_intensity_sum</th>\n",
       "      <th>spot_intensity_std</th>\n",
       "      <th>other_spot_intensity</th>\n",
       "      <th>bg_intensity</th>\n",
       "      <th>far_bg_intensity</th>\n",
       "      <th>intensity_vs_far_bg_ratio</th>\n",
       "      <th>intensity_vs_other_spots_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.373789e+08</td>\n",
       "      <td>68.115942</td>\n",
       "      <td>68.115942</td>\n",
       "      <td>63.768116</td>\n",
       "      <td>0.249780</td>\n",
       "      <td>0.339192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68.115942</td>\n",
       "      <td>2</td>\n",
       "      <td>237378.921875</td>\n",
       "      <td>237378.921875</td>\n",
       "      <td>1.637915e+07</td>\n",
       "      <td>289332.531250</td>\n",
       "      <td>23.364450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.373789e+08</td>\n",
       "      <td>10159.398482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312912</td>\n",
       "      <td>0.878816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>165.844162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.872822e+00</td>\n",
       "      <td>1.049917</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>84.057971</td>\n",
       "      <td>89.270422</td>\n",
       "      <td>80.242060</td>\n",
       "      <td>94.245650</td>\n",
       "      <td>94.117647</td>\n",
       "      <td>1.051330</td>\n",
       "      <td>170</td>\n",
       "      <td>8511.894531</td>\n",
       "      <td>6315.216309</td>\n",
       "      <td>5.873208e+05</td>\n",
       "      <td>7861.288574</td>\n",
       "      <td>1859.231934</td>\n",
       "      <td>2197.852051</td>\n",
       "      <td>2196.678223</td>\n",
       "      <td>3.874892e+00</td>\n",
       "      <td>4.578175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.495195e+00</td>\n",
       "      <td>0.995450</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>88.405797</td>\n",
       "      <td>99.385156</td>\n",
       "      <td>99.252236</td>\n",
       "      <td>99.457056</td>\n",
       "      <td>99.466830</td>\n",
       "      <td>0.995353</td>\n",
       "      <td>189</td>\n",
       "      <td>1390.449829</td>\n",
       "      <td>460.323975</td>\n",
       "      <td>9.594104e+04</td>\n",
       "      <td>655.091492</td>\n",
       "      <td>1065.456055</td>\n",
       "      <td>929.944519</td>\n",
       "      <td>930.125854</td>\n",
       "      <td>1.494903e+00</td>\n",
       "      <td>1.305027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.027741e+00</td>\n",
       "      <td>0.990267</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.980786</td>\n",
       "      <td>99.976873</td>\n",
       "      <td>99.982899</td>\n",
       "      <td>99.982519</td>\n",
       "      <td>0.990270</td>\n",
       "      <td>189</td>\n",
       "      <td>10424.539062</td>\n",
       "      <td>273.745117</td>\n",
       "      <td>7.192932e+05</td>\n",
       "      <td>1288.304321</td>\n",
       "      <td>9895.628906</td>\n",
       "      <td>10143.155273</td>\n",
       "      <td>10150.793945</td>\n",
       "      <td>1.026968e+00</td>\n",
       "      <td>1.053449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   on_off_ratio  occupancy_ratio  spot_occupancy  spot_occupancy_thresholded  \\\n",
       "0  2.373789e+08        68.115942       68.115942                   63.768116   \n",
       "1  0.000000e+00         0.000000        0.000000                    0.000000   \n",
       "2  3.872822e+00         1.049917      100.000000                   84.057971   \n",
       "3  1.495195e+00         0.995450      100.000000                   88.405797   \n",
       "4  1.027741e+00         0.990267      100.000000                  100.000000   \n",
       "\n",
       "   image_occupancy  other_spots_occupancy  bg_occupancy  far_bg_occupancy  \\\n",
       "0         0.249780               0.339192      0.000000          0.000000   \n",
       "1         0.312912               0.878816      0.000000          0.000000   \n",
       "2        89.270422              80.242060     94.245650         94.117647   \n",
       "3        99.385156              99.252236     99.457056         99.466830   \n",
       "4        99.980786              99.976873     99.982899         99.982519   \n",
       "\n",
       "   occupancy_vs_far_bg_ratio  in_n_spots  spot_intensity  \\\n",
       "0                  68.115942           2   237378.921875   \n",
       "1                   0.000000           3        0.000000   \n",
       "2                   1.051330         170     8511.894531   \n",
       "3                   0.995353         189     1390.449829   \n",
       "4                   0.990270         189    10424.539062   \n",
       "\n",
       "   spot_intensity_bgr_corrected  spot_intensity_sum  spot_intensity_std  \\\n",
       "0                 237378.921875        1.637915e+07       289332.531250   \n",
       "1                      0.000000        0.000000e+00            0.000000   \n",
       "2                   6315.216309        5.873208e+05         7861.288574   \n",
       "3                    460.323975        9.594104e+04          655.091492   \n",
       "4                    273.745117        7.192932e+05         1288.304321   \n",
       "\n",
       "   other_spot_intensity  bg_intensity  far_bg_intensity  \\\n",
       "0             23.364450      0.000000          0.000000   \n",
       "1            165.844162      0.000000          0.000000   \n",
       "2           1859.231934   2197.852051       2196.678223   \n",
       "3           1065.456055    929.944519        930.125854   \n",
       "4           9895.628906  10143.155273      10150.793945   \n",
       "\n",
       "   intensity_vs_far_bg_ratio  intensity_vs_other_spots_ratio  \n",
       "0               2.373789e+08                    10159.398482  \n",
       "1               0.000000e+00                        0.000000  \n",
       "2               3.874892e+00                        4.578175  \n",
       "3               1.494903e+00                        1.305027  \n",
       "4               1.026968e+00                        1.053449  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_far_bg(mask, bg):\n",
    "    \"\"\"Gets mask for background pixels that are at least 4 radii away from the spot\"\"\"\n",
    "    # 3 iterations = (1+3=)4x the spot radius\n",
    "    expanded_spot = binary_dilation(mask, crop_zeros(mask), iterations=3)\n",
    "    return bg & ~expanded_spot\n",
    "\n",
    "def occ(px):\n",
    "    \"\"\"Calculates non-zero % of the given array\"\"\"\n",
    "    return (np.count_nonzero(px) / px.size)*100\n",
    "\n",
    "metrics = []\n",
    "for row in merged_df.itertuples():\n",
    "    grid = grids[row.dataset_name]\n",
    "    \n",
    "    mask = grid == row.well\n",
    "    bg = grid == 0\n",
    "    far_bg = calc_far_bg(mask, bg)\n",
    "        \n",
    "    in_mask = row.image[mask]\n",
    "    in_bg = row.image[bg]\n",
    "    in_far_bg = row.image[far_bg]\n",
    "    in_other_spots = row.image[~bg & ~mask]\n",
    "    \n",
    "    # Calculate threshold (0.01 * 99th percentile) \n",
    "    # (note the image is already hotspot-removed, so the max is the 99th percentile)\n",
    "    threshold = np.max(row.image) * 0.01\n",
    "\n",
    "    metrics.append({\n",
    "        # Original metrics\n",
    "        # NOTE: The constant in the denominator of `on_off_ratio` was changed to\n",
    "        # 0.001 as it seemed to produce slightly better results\n",
    "        'on_off_ratio': (np.mean(in_mask)) / (np.mean(in_bg) + 0.001),\n",
    "        'occupancy_ratio': occ(in_mask) / (occ(in_bg) + 1),\n",
    "        \n",
    "        # Single-spot occupancy %\n",
    "        'spot_occupancy': occ(in_mask),\n",
    "        'spot_occupancy_thresholded': occ(in_mask > threshold),\n",
    "        # Other occupancy metrics\n",
    "        'image_occupancy': occ(row.image),\n",
    "        'other_spots_occupancy': occ(in_other_spots),\n",
    "        'bg_occupancy': occ(in_bg),\n",
    "        'far_bg_occupancy': occ(in_far_bg),\n",
    "        'occupancy_vs_far_bg_ratio' : occ(in_mask) / (occ(in_far_bg) + 1),\n",
    "        \n",
    "        # How many spots have a non-zero pixel\n",
    "        'in_n_spots': len(np.unique(grid[(grid != 0) & (row.image > threshold)])),\n",
    "        \n",
    "        # Intensity merics\n",
    "        'spot_intensity' : np.mean(in_mask),\n",
    "        'spot_intensity_bgr_corrected' : np.mean(in_mask) - np.mean(in_far_bg),\n",
    "        'spot_intensity_sum' : np.sum(in_mask),\n",
    "        'spot_intensity_std' : np.std(in_mask),\n",
    "        'other_spot_intensity': np.mean(in_other_spots),\n",
    "        'bg_intensity' : np.mean(in_bg),\n",
    "        'far_bg_intensity' : np.mean(in_far_bg),\n",
    "        # Intensity ratios\n",
    "        'intensity_vs_far_bg_ratio': np.mean(in_mask) / (np.mean(in_far_bg) + 0.001),\n",
    "        'intensity_vs_other_spots_ratio': np.mean(in_mask) / (np.mean(in_other_spots) + 0.001),\n",
    "\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, index=merged_df.index)\n",
    "metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-02T15:54:16.477074Z",
     "start_time": "2021-06-02T15:53:05.276Z"
    }
   },
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(verbose=False)\n",
    "model.load_model(p_model, format='json')\n",
    "\n",
    "features = ['spot_occupancy_thresholded', 'occupancy_vs_far_bg_ratio', 'intensity_vs_far_bg_ratio', 'intensity_vs_other_spots_ratio']\n",
    "\n",
    "# Make predictions for all data\n",
    "predictions_df = pd.DataFrame({\n",
    "    'pred_val': model.predict_proba(metrics_df[features].values)[:, 1]\n",
    "}, index=metrics_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make combined DF\n",
    "output_df = merged_df.join(metrics_df).join(predictions_df)\n",
    "\n",
    "# Add two-state and three-state classes\n",
    "output_df['pred_twostate'] = np.where(output_df.pred_val < 0.5, 0, 1)\n",
    "unsure_range = [0.2, 0.8] # Lowest & highest values to include in the \"unsure\" class\n",
    "# This assigns 0 = negative, 1 = unsure, 2 = positive\n",
    "output_df['pred_threestate'] = np.digitize(output_df.pred_val, unsure_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%d-%b-%Y\") \n",
    "\n",
    "csv_df = output_df.drop(columns=['image', 'filename']) # Skip unwanted columns\n",
    "csv_df.to_csv(p_root_dir / f\"5_data_analysis/all_predictions_{timestamp}.csv\")\n",
    "\n",
    "for dataset_name, results_df in csv_df.groupby('dataset_name'):\n",
    "    output_path = p_eval / f'{dataset_name}_predictions.csv'\n",
    "    results_df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean output directories\n",
    "# for output_path in [\n",
    "#     p_eval_upos, p_eval_uneg, \n",
    "#     p_tri_pos, p_tri_unk, p_tri_neg\n",
    "# ]:\n",
    "#     output_path.mkdir(parents=True, exist_ok=True)\n",
    "#     for f in output_path.glob('*.png'):\n",
    "#         f.unlink()  # Delete existing files\n",
    "\n",
    "# Write images with two-state classification\n",
    "for row in output_df.itertuples():\n",
    "    mask = grids[row.dataset_name] == row.well\n",
    "    \n",
    "    twostate_path = [p_eval_uneg, p_eval_upos][row.pred_twostate]\n",
    "    \n",
    "    save_image_with_mask(row.image, mask, twostate_path / row.filename)\n",
    "    \n",
    "# Write images with three-state classification\n",
    "for row in output_df.itertuples():\n",
    "    mask = grids[row.dataset_name] == row.well\n",
    "    \n",
    "    threestate_path = [p_tri_neg, p_tri_unk, p_tri_pos][row.pred_threestate]\n",
    "    \n",
    "    save_image_with_mask(row.image, mask, threestate_path / row.filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
