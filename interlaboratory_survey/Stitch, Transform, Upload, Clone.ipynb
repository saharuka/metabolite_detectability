{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "virtual-harvey",
   "metadata": {},
   "source": [
    "# Concatenate low mass range and high mass range datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from concatenate_imzml_files import concat_imzml_files\n",
    "from concatenate_imzml_files import transform_imzml\n",
    "from definitions import ROOT_DIR\n",
    "import numpy as np\n",
    "from metaspace.sm_annotation_utils import SMInstance\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a5236",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_imzml?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-conversation",
   "metadata": {},
   "source": [
    "Provide inputs\n",
    "- Folder where imzmls for all matrices are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(ROOT_DIR) / \"1_stitch_and_upload_datasets\"\n",
    "p_out = p / 'Missing'\n",
    "print(p_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea74b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into metaspace\n",
    "sm = SMInstance(host='https://metaspace2020.eu')\n",
    "\n",
    "if not sm.logged_in():\n",
    "    # Using getpass here prevents the API key from being accidentally saved with this notebook.\n",
    "    api_key = getpass.getpass(prompt='API key: ', stream=None)\n",
    "    sm.login(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e3462",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = pd.read_csv(p / 'missing.csv') #List of datasets + metadata to grab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa0e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_id in datasets['ds_id']:\n",
    "    print(dataset_id)\n",
    "    #ds = sm.dataset(id = dataset_id)\n",
    "    #ds.download_to_dir(p_out,dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-steering",
   "metadata": {},
   "source": [
    "- Iterate through csv file and merge all labelled pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-duncan",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Function to merge two aligned imzml files\n",
    "\n",
    "dsets = (datasets[datasets['Mergewith'] != \"0\"]['ds_id'])\n",
    "for first in dsets:\n",
    "    ds = datasets[datasets['ds_id'] == first]\n",
    "    slide = ds['Slide'].item()\n",
    "    second = ds['Mergewith'].item()\n",
    "    polarity = ds['Polarity'].item()\n",
    "    rotation = ds['Rotation'].item()\n",
    "    group = ds['Group'].item()\n",
    "    done = ds['Done'].item()\n",
    "    \n",
    "    input_paths = [p_out / (first+\".imzml\"), p_out / (second+'.imzml')]\n",
    "    output_path = p_out / f'{group}_{slide}_{polarity}.imzml'\n",
    "    offsets = [(0,0), (0,0)] # (x,y) tuple for each imzML file if you want to offset either file's coordinates\n",
    "    mz_ranges = [('auto','auto'), ('auto','auto')] # (min_mz, max_mz) tuples. Replace 'auto' with a non-string number if you want to explicitly specify the m/z range for each file\n",
    "    if (done == 0):\n",
    "        concat_imzml_files(input_paths, offsets, mz_ranges, output_path, rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8010cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to rotate unmerged dataset\n",
    "\n",
    "dsets = (datasets[datasets['Mergewith'] == \"0\"]['ds_id'])\n",
    "for first in dsets:\n",
    "    ds = datasets[datasets['ds_id'] == first]\n",
    "    slide = ds['Slide'].item()\n",
    "    polarity = ds['Polarity'].item()\n",
    "    rotation = ds['Rotation'].item()\n",
    "    group = ds['Group'].item()\n",
    "    done = ds['Done'].item()\n",
    "    mrange = ds['Mass Range'].item()\n",
    "    \n",
    "    input_path = p_out / (first+\".imzml\")\n",
    "    output_path = p_out / f'{group}_{slide}_{polarity}.imzml'\n",
    "    if (done == 0 and mrange == \"Full\"):\n",
    "        transform_imzml(input_path, output_path, rotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-marble",
   "metadata": {},
   "source": [
    "- Concatenate each pair of files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-tractor",
   "metadata": {},
   "source": [
    "# Upload stitched datasets to METASPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, getpass\n",
    "from metaspace import SMInstance\n",
    "import pandas as pd\n",
    "from definitions import ROOT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-spectrum",
   "metadata": {},
   "source": [
    "- To avoid processing with HMDB, upload on staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMInstance(host='https://metaspace2020.eu')\n",
    "\n",
    "if not sm.logged_in():\n",
    "    # Using getpass here prevents the API key from being accidentally saved with this notebook.\n",
    "    api_key = getpass.getpass(prompt='API key: ', stream=None)\n",
    "    sm.login(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-commitment",
   "metadata": {},
   "source": [
    "Provide inputs, for each pair:\n",
    "\n",
    "matrix full name, additional neutral gain just for this matrix (matrix molecule), solvent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_fullname = dict({\n",
    "    'DHB':['2,5-dihydroxybenzoic acid', ['+C7H6O4'], 'ACN (70% v/v, aq.)'],\n",
    "    'DAN':['1,5-diaminonaphthalene', ['+C10H10N2', '+C10H8N2'], 'ACN (70% v/v, aq.)'],\n",
    "    'norharmane':['norharmane', ['+C11H8N2'], 'CHCl3:MeOH (1:1 v/v)'],\n",
    "    '9AA':['9-aminoacridine', ['+C13H10N2'], 'MeOH (70% v/v, aq.)'],\n",
    "    'CHCA':['alpha-cyano-4-hydroxycinnamic acid', ['+C10H7NO3'], 'ACN (50% v/v, aq.)'],\n",
    "    'ClCCA':['4-chloro-alpha-cyanocinnamic acid', ['+C10H6ClNO2'], 'ACN (50% v/v, aq.)'],\n",
    "    'NEDC':['N-(1-naphthyl)ethylenediamine dihydrochloride', ['+C12H14N2', '+HCl'], 'ACN (70% v/v, aq.)'],\n",
    "    'PNDIT2':['PNDI-T2', [], 'Toluene'],\n",
    "    'MAPS':['Maleic anhydride proton sponge', ['+C18H18N2O3'], 'Toluene'],\n",
    "    'DHAP': ['2,5-dihydroxyacetophenone', ['+C8H8O3'], 'ACN (70% v/v, aq.)'],\n",
    "    'pNA' : ['4-Nitroaniline', ['+C6H6N2O2'], 'MeOH (85% v/v, aq.)'],\n",
    "    'CMBT': ['5-Chloro-2-mercaptobenzothiazole', ['+C7H4ClNS2'], 'ACN (90% v/v, aq.)'],\n",
    "    'None' : ['none', [], 'none'],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-tuning",
   "metadata": {},
   "source": [
    "- Define neutral losses in the function below\n",
    "\n",
    "Might be of interest for interpreting neutral losses http://www.colby.edu/chemistry/PChem/StableLoss.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_dataset(dataset_name, matrix, solvent, polarity, adducts, rp, mref, source, analyzer, px_x, px_y, extra_neutral_losses = [], databases = [('Spotting_project_compounds-v9', 'feb2021')], is_public = False):\n",
    "\n",
    "    metadata = {\n",
    "        'Data_Type': 'Imaging MS',  # shouldn't be changed\n",
    "        'Sample_Information': {\n",
    "            'Organism': 'None',\n",
    "            'Organism_Part': 'None',\n",
    "            'Condition': 'None',\n",
    "            'Sample_Growth_Conditions': 'None'  # this is an extra field\n",
    "        },\n",
    "        'Sample_Preparation': {\n",
    "            'Sample_Stabilisation': 'None',\n",
    "            'Tissue_Modification': 'None',\n",
    "            'MALDI_Matrix': matrix,\n",
    "            'MALDI_Matrix_Application': 'N/A',\n",
    "            'Solvent': solvent\n",
    "        },\n",
    "        'MS_Analysis': {\n",
    "            'Polarity': polarity,\n",
    "            'Ionisation_Source': source,\n",
    "            'Analyzer': analyzer,\n",
    "            'Detector_Resolving_Power': {\n",
    "                'mz': mref,\n",
    "                'Resolving_Power': rp\n",
    "            },\n",
    "            'Pixel_Size': {\n",
    "                'Xaxis': px_x,\n",
    "                'Yaxis': px_y\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    ds_id  = sm.submit_dataset(\n",
    "    imzml_fn, ibd_fn, dataset_name,\n",
    "    json.dumps(metadata), is_public, databases,\n",
    "    project_ids=['62d1990a-a4ff-11eb-96db-abcc9848804b'],\n",
    "    adducts=adducts,\n",
    "    neutral_losses = ['-H2O', '-H2', '+H2', #redox\n",
    "                      '-CO2', '-CH2O3', '-CH2O2', # CO2+H2O, formic acid?\n",
    "                      '-HPO3', '-H3PO4', # phosphate\n",
    "                      '-NH3',# '-C2H5NO2',  # glycine\n",
    "                     ] + extra_neutral_losses,\n",
    "    ppm=10        \n",
    "    )\n",
    "    \n",
    "    return ds_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f15ceb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id_list = []\n",
    "\n",
    "for ds_id in dsets:\n",
    "    \n",
    "    ds = datasets[datasets['ds_id'] == ds_id]\n",
    "    slide = ds['Slide'].item()\n",
    "    pol_short = ds['Polarity'].item()\n",
    "    mat_short = ds['Matrix'].item()\n",
    "    rp = ds['RP'].item()\n",
    "    mref = ds['Mref'].item()\n",
    "    group = ds['Group'].item()\n",
    "    source = ds['Source'].item()\n",
    "    analyzer = ds['Analyzer'].item()\n",
    "    px_x = ds['Px_x'].item()\n",
    "    px_y = ds['Px_y'].item()\n",
    "    \n",
    "    if(group != \"Dreisewerd\"):\n",
    "    \n",
    "        matrix = matrix_fullname[mat_short][0]\n",
    "        extra_neutral_losses = matrix_fullname[mat_short][1]\n",
    "        imzml_fn = p_out / f'{group}_{slide}_{pol_short}.imzml'\n",
    "        ibd_fn = p_out / f'{group}_{slide}_{pol_short}.ibd'\n",
    "        dataset_name = f'{group}_{slide}_{pol_short}'\n",
    "        solvent = 'N/A'\n",
    "        if pol_short == \"neg\":\n",
    "            polarity = 'Negative'\n",
    "            adducts = ['[M]-', '-H', '+Cl']\n",
    "        else:\n",
    "            polarity = 'Positive'\n",
    "            adducts = ['[M]+', '+H', '+Na', '+K']\n",
    "\n",
    "        print(\"Hello, \"+f'{group}_{slide}_{pol_short}.imzml: '+matrix+f', {px_x}:{px_y}')\n",
    "        submit_dataset(dataset_name, matrix, solvent, polarity, adducts, rp, mref, source, analyzer, px_x, px_y, extra_neutral_losses)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dab9090",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = pd.DataFrame(id_list)\n",
    "id_list.to_csv(p / 'uploaded_interlab_datasets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-consolidation",
   "metadata": {},
   "source": [
    "- Upload datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(ROOT_DIR) / \"1_stitch_and_upload_datasets\"\n",
    "\n",
    "matrices = []\n",
    "modes = []\n",
    "ds_names = []\n",
    "ids = []\n",
    "losses = []\n",
    "\n",
    "for mpath in p.iterdir():\n",
    "    if mpath == p /'pNA':\n",
    "#     if mpath.is_dir():\n",
    "        for pol in ['pos', 'neg']:\n",
    "            imzml_fn = list(mpath.rglob(f\"*{pol}*mz70-1510*.imzML\"))[0]\n",
    "            ibd_fn = list(mpath.rglob(f\"*{pol}*mz70-1510*.ibd\"))[0]\n",
    "            dataset_name = imzml_fn.name\n",
    "            matrix = matrix_fullname[mpath.name][0]\n",
    "            extra_neutral_losses = matrix_fullname[mpath.name][1]\n",
    "            solvent = matrix_fullname[mpath.name][2]\n",
    "            \n",
    "            if pol == 'pos':\n",
    "                polarity = 'Positive'\n",
    "                adducts = ['[M]+', '+H', '+Na', '+K']\n",
    "            else: \n",
    "                polarity = 'Negative'\n",
    "                adducts = ['[M]-', '-H', '+Cl']\n",
    "            \n",
    "            ds_id = submit_dataset(dataset_name, matrix, solvent, polarity, adducts, extra_neutral_losses)\n",
    "\n",
    "            matrices.append(matrix)\n",
    "            modes.append(polarity)\n",
    "            ds_names.append(dataset_name)\n",
    "            ids.append(ds_id)\n",
    "            losses.append(extra_neutral_losses)\n",
    "            \n",
    "df = pd.DataFrame({\n",
    "    'matrix':matrices,\n",
    "    'polarity':modes,\n",
    "    'ds_name':ds_names,\n",
    "    'ds_id':ids,\n",
    "    'extra_neutral_losses':losses\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6209a",
   "metadata": {},
   "source": [
    "# Function for cloning datasets that are too big to upload through API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20483dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_dataset(dataset, name, extra_losses):\n",
    "    ds = sm.dataset(id=dataset)\n",
    "    ds_config = ds.config\n",
    "    # The following lines modify the existing config before sending it back\n",
    "    # Remove any lines for fields where an override isn't needed\n",
    "    #ds_config['analysis_version'] = 2\n",
    "    ds_config['image_generation']['ppm'] = 10\n",
    "    #ds_config['isotope_generation']['chem_mods'] = []\n",
    "    ds_config['isotope_generation']['neutral_losses'] = ['-H2O', '-H2', '+H2', #redox\n",
    "                          '-CO2', '-CH2O3', '-CH2O2', # CO2+H2O, formic acid?\n",
    "                          '-HPO3', '-H3PO4', # phosphate\n",
    "                          '-NH3',# '-C2H5NO2',  # glycine\n",
    "                         ] + extra_losses\n",
    "    \n",
    "    if ds_config['isotope_generation']['charge'] > 0: # Check polarity to decide which adducts to use\n",
    "        ds_config['isotope_generation']['adducts'] = ['[M]+', '+H', '+Na', '+K']\n",
    "    else:\n",
    "        ds_config['isotope_generation']['adducts'] = ['[M]-', '-H', '+Cl']\n",
    "\n",
    "    # Add Pixel_Size if it's missing (only a problem with very old datasets)\n",
    "    if 'Pixel_Size' not in ds.metadata['MS_Analysis']:\n",
    "        ds.metadata['MS_Analysis']['Pixel_Size'] = {\n",
    "            'Xaxis': 100,\n",
    "            'Yaxis': 100,\n",
    "        }\n",
    "\n",
    "    # Use original databases\n",
    "    #databases = [(db.name, db.version) for db in ds.database_details]\n",
    "    \n",
    "    # or override them:\n",
    "    databases = [('Spotting_project_compounds-v9', 'feb2021')]\n",
    "    \n",
    "    new_dataset = sm.submit_dataset(\n",
    "        imzml_fn=None,\n",
    "        ibd_fn=None,\n",
    "        name=name,\n",
    "        metadata=ds.metadata,\n",
    "        is_public=False,\n",
    "        databases=databases,\n",
    "        project_ids=['62d1990a-a4ff-11eb-96db-abcc9848804b'],  # Add a project ID here if desired, otherwise delete this line\n",
    "        adducts=ds_config['isotope_generation']['adducts'],\n",
    "        neutral_losses = ds_config['isotope_generation']['neutral_losses'],\n",
    "        ppm = ds_config['image_generation']['ppm'],\n",
    "        input_path=ds.s3dir,\n",
    "    )\n",
    "    print(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f0f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsids = datasets['ds_id'][0:4]\n",
    "for dsid in dsids:\n",
    "    dset = datasets[datasets.ds_id==dsid]\n",
    "    name = dset['Group'].item()+\"_\"+dset['Slide'].item()+\"_\"+dset['Polarity'].item()+\"_\"+dset['Mass Range'].item()\n",
    "    extra_losses = matrix_fullname[dset['Matrix'].item()][1]\n",
    "    print(dset)\n",
    "    clone_dataset(dsid, name, extra_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-consequence",
   "metadata": {},
   "source": [
    "- Save information about uploaded dataset including dataset id on staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-suspension",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(p / 'uploaded_datasets.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
